\section{Conclusion}
\label{sec:conclusions}

To conclude, we improve the original algorithm by two means. The first one is to remove some relaxation steps in a V-cycle. This leads to faster cycles but it converges in more cycles. All in all,
   it is usually faster (up to 30\% according to our experiments) but the improvement can be very variable. The second way to improve the algorithm is to adapt the precisions of the floating point
   operations depending on which accuracy we already got. We use small precisions for the first cycles then when it starts converging, we increase this precision. Repeating this operation until reaching the maximum accuracy available, leads
   to some economy in some steps of the algorithm, making it faster overall. Then by combining these two techniques, we estimate than using a GPU, we can reduce by 16.4\% the time needed to reach the maximum accuracy using double-precision and by 14.5\% the time needed
   to reach the maximum accuracy using single-precision.

   Other ideas to reduce the execution time or increase the convergence could include changing the precision used in different levels of a cycle. However, we think that it is not
   useful because (1) using a greater precision in the coarse levels than in the fine levels is useless as all the computations would eventually be truncated and (2) using a smaller
   precision in the coarse levels than in the fine levels would not affect by much the execution time as, according to the measurements done, only the first 2 or 3 levels represent more than 95\% of the relaxation cost of a cycle.
   Finally, it would be interesting to estimate the impact of our algorithm on the energy consumption, or at least have some real measurements and evaluate the energy consumption with our
   same model but a different value of the $\alpha$ parameter.
